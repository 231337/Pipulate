{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipulate Functions: Stuff you can do from a Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions for conducting SEO and other data investigations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests, re, sys, os         #Modules that should be available to ALL pipulate functions\n",
    "from collections import namedtuple   #Allows creation of a highly readable response object API for functions\n",
    "from html.parser import HTMLParser   #Needed for MLStripper to inherit from\n",
    "from urllib.parse import urlparse    #Very commonly used for getting the parts out of a URL\n",
    "import notebook_finder               #Allows importing of other .ipynb files as if they were .py file modules\n",
    "import goodsheet                     #Provides OAuth2 login for Google Sheets and other Google Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define standard for Pipulate functions to return their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Response = namedtuple('Response', 'ok status_code text') #Standard API for pipulate response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipulate funcs return Response(ok=True, status_code='200', text='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(**kwargs):\n",
    "    \"\"\"This is a bare-bones copy-and-paste example for new Pipulate functions.\n",
    "    \n",
    "    Functions in this file can be used as column names in Google Sheets.\n",
    "    Text output from these functions get inserted into the spreadsheet.\n",
    "    Pipulate functions must have **kwargs unless preceded by a decorator.\n",
    "    Decorators and support-functions significantly simplify these functions.\n",
    "    The minimum a Pipulate function must return is this 3-element tuple.\"\"\"\n",
    "    return Response(ok=True, status_code='200', text='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def url(passed_in_func):\n",
    "    \"\"\"This is the frequently-used decorator function that passes along pre-fetched HTML.\n",
    "    \n",
    "    This decorator function allows you to use @url above any Pipulate function.\n",
    "    This in turn allows \"html\" to be used as the argument instead of **kwargs.\n",
    "    Swapping out full HTML for a URL is great for screen-scraping functions like title.\n",
    "    This is also very efficient, because all URL fetches are cached in a database.\n",
    "    This means that multiple scraper functions can be used together efficiently.\n",
    "    This function must appear in this file before anything decorated by it.\"\"\"\n",
    "    def requests_wrapper(**row_dict):\n",
    "        html = row_dict['response'].text\n",
    "        return passed_in_func(html=html)\n",
    "    return requests_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@url\n",
    "def Title(html):\n",
    "    \"\"\"This is the quintessential example of grabbing a title tag from a URL.\n",
    "\n",
    "    This function does not need **kwargs as it's argument BECAUSE it's decorated.\n",
    "    The @url decorator pre-filters **kwargs and sends along only the pre-fetched html.\n",
    "    The HTML is cached for efficiency on subsequent calls of the same URL.\n",
    "    This pattern can be used for extracting any TEXT NODE from HTML (not attributes).\n",
    "    Pipulate functions are case insensitive, so this could also be 'title'.\"\"\"\n",
    "    return extract_text_node(html=html, tag='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def realurl(**row_dict):\n",
    "    \"\"\"Returns the actual URL retrieved after resolving all redirects.\"\"\"\n",
    "    url = row_dict['url']\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        return Response(ok=True, status_code='200', text=response.url)\n",
    "    except:\n",
    "        return Response(ok=True, status_code='200', text=response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apexdomain(**row_dict):\n",
    "    \"\"\"Usually returns the apex or registered domain, given an URL.\"\"\"\n",
    "    path = row_dict['url']\n",
    "    if path:\n",
    "        apex = urlparse(path).hostname.split(\".\")\n",
    "        try:\n",
    "            apex = \".\".join(len(apex[-2]) < 4 and apex[-3:] or apex[-2:])\n",
    "            return Response(ok=True, status_code='200', text=apex)\n",
    "        except:\n",
    "            return Response(ok=True, status_code='200', text=\"Can't find\")\n",
    "    else:\n",
    "        return Response(ok=True, status_code='400', text='No input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gaorganic(**kwargs):\n",
    "    path = urlparse(kwargs['url']).path\n",
    "    path = \"ga:pagePath==%s\" % path.replace(\",\", \"\\,\")\n",
    "    service = create_google_service(api_name=\"analytics\", version=\"v3\")\n",
    "    ga_request = service.data().ga().get(\n",
    "        ids=kwargs['ids'],\n",
    "        start_date=kwargs['startdate'],\n",
    "        end_date=kwargs['enddate'],\n",
    "        metrics='ga:organicSearches',\n",
    "        dimensions='ga:pagePath',\n",
    "        filters=path,\n",
    "        start_index='1',\n",
    "        max_results='100'\n",
    "    )\n",
    "    try:\n",
    "        ga_response = ga_request.execute()\n",
    "    except:\n",
    "        return Response(ok=False, status_code='500', text=\"Did not execute\")\n",
    "    if ga_response and 'rows' in ga_response:\n",
    "        return Response(ok=True, status_code='200', text=ga_response['rows'][0][1])\n",
    "    else:\n",
    "        return Response(ok=False, status_code='200', text='Not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate() funcs return lists of lists [['A1','B1','C1'],['A2','B2','C2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def populate_from_gsc(site, start, end):\n",
    "    service = create_google_service(api_name=\"webmasters\", version=\"v3\")\n",
    "    listoflists = []\n",
    "    units = 5000\n",
    "    for aset in range(0, units*4, units):\n",
    "        mydata = None\n",
    "        request = {\n",
    "            'startDate': str(start),\n",
    "            'endDate': str(end),\n",
    "            'dimensions': ['query', 'page'],\n",
    "            'rowLimit': str(units),\n",
    "            'startRow': str(aset)\n",
    "        }\n",
    "        mydata = service.searchanalytics().query(siteUrl=site, body=request).execute()\n",
    "        if mydata and 'rows' in mydata:\n",
    "            listoflists.append(mydata['rows'])\n",
    "        else:\n",
    "            break\n",
    "    too_many_dicts = sum(listoflists, [])\n",
    "    list_of_rows = []\n",
    "    list_of_rows.append(['keyword', 'url', 'position', 'impressions', 'clicks', 'ctr'])\n",
    "    for index, item in enumerate(too_many_dicts):\n",
    "        row = [item['keys'][0], item['keys'][1], item['position'], \n",
    "               item['impressions'], item['clicks'], item['ctr']]\n",
    "        list_of_rows.append(row)    \n",
    "    return list_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def populate_from_ga(prepath, profileid, start, end):\n",
    "    \"\"\"Experiment at https://ga-dev-tools.appspot.com/query-explorer/\"\"\"\n",
    "    service = create_google_service(api_name=\"analytics\", version=\"v3\")\n",
    "    ga_request = service.data().ga().get(\n",
    "        ids=profileid,\n",
    "        start_date=start,\n",
    "        end_date=end,\n",
    "        metrics='ga:organicSearches,ga:sessions,ga:bounces',\n",
    "        dimensions='ga:pagePath',\n",
    "        sort='-ga:organicSearches,-ga:sessions',\n",
    "        filters='ga:organicSearches>0',\n",
    "        samplingLevel='HIGHER_PRECISION',\n",
    "        start_index='1',\n",
    "        max_results='10000'\n",
    "    )\n",
    "    try:\n",
    "        ga_response = ga_request.execute()\n",
    "    except:\n",
    "        return [['ERROR']]\n",
    "    if 'rows' in ga_response:\n",
    "        raw_rows = ga_response['rows']\n",
    "        list_of_lists = [['url', 'organicsearches','sessions','bounces']]+[[prepath+a,b,c,d] for a,b,c,d in raw_rows]\n",
    "        return list_of_lists\n",
    "    else:\n",
    "        return [['ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_page_crawl(url, regex=None):\n",
    "    \"\"\"Returns a set of all site URLs found on a page, optionally filtered.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "    except:\n",
    "        return [['ERROR']]\n",
    "    lookfor = urlparse(url).hostname\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    all_links = soup.find_all(\"a\")\n",
    "    nodupes = set()\n",
    "    for link in all_links:\n",
    "        href = link.get(\"href\")\n",
    "        if (type(href).__name__ == 'str' and len(href) > len(lookfor)\n",
    "            and lookfor in href and href not in nodupes):\n",
    "            if regex:\n",
    "                match = re.search(regex, href, flags=re.I)\n",
    "                if match:\n",
    "                    nodupes.add(href)\n",
    "            elif not regex:\n",
    "                nodupes.add(href)\n",
    "    if nodupes:\n",
    "        columns = [['URL']]\n",
    "        list_of_lists = [[x] for x in nodupes]\n",
    "        return columns + list_of_lists\n",
    "    else:\n",
    "        return [['ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions that are not called from sheets or workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text_node(html, tag):\n",
    "    \"\"\"This is the helper function that extracts text-nodes like title tags from HTML.\n",
    "    \n",
    "    This function is for simple text-node screen scraping, such as for the Title tag.\n",
    "    It is different from many support functions in that the return values get wrapped\n",
    "    in the same Response() named tuple as functions that are meant to be called from \n",
    "    spreadsheets directly (like Title), so that those can be short and clear like this:\n",
    "    \n",
    "        @url\n",
    "        def Title(html):\n",
    "            return extract_text_node(html=html, tag=\"title\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if not html or not tag:\n",
    "        return Response(ok=False, status_code='400', text=None)\n",
    "    pattern = r'<{0}\\s?>(.*?)</{0}\\s?>'.format(tag.lower())\n",
    "    compiled = re.compile(pattern=pattern, flags=re.DOTALL)\n",
    "    matches = compiled.findall(string=html)\n",
    "    if matches:\n",
    "        text = matches[0].strip()\n",
    "        return Response(ok=True, status_code='200', text=text)\n",
    "    else:\n",
    "        return Response(ok=True, status_code='200', text=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_google_service(api_name, version):\n",
    "    \"\"\"This lets you create instances of Google Services.\"\"\"\n",
    "    import httplib2\n",
    "    from oauth2client import file, tools\n",
    "    from apiclient.discovery import build\n",
    "    path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    path_filename = os.path.join(path, goodsheet.filename)\n",
    "    storage = file.Storage(path_filename)\n",
    "    credentials = storage.get()\n",
    "    http = credentials.authorize(http = httplib2.Http())\n",
    "    service = build(api_name, version, http)\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    \"\"\"http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "    \n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def normalize_whitespace(string):\n",
    "    return re.sub('[\\s\\r\\n]+', ' ', string)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
