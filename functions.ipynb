{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipulate Functions: Stuff you can do from a Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions for conducting SEO and other data investigations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, sys, os, shelve\n",
    "import requests\n",
    "from collections import namedtuple\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse, quote, quote_plus, urljoin\n",
    "import notebook_finder\n",
    "import goodsheet\n",
    "import private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Values for Functions scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Response = namedtuple('Response', 'ok status_code text') #For Pipulate functions that send values to cells\n",
    "proxy, proxies = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipulate funcs return Response(ok=True, status_code='200', text='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(**kwargs):\n",
    "    \"\"\"This is a bare-bones copy-and-paste example for new Pipulate functions.\n",
    "    \n",
    "    Functions in this file can be used as column names in Google Sheets.\n",
    "    Text output from these functions get inserted into the spreadsheet.\n",
    "    Pipulate functions must have **kwargs unless preceded by a decorator.\n",
    "    Decorators and support-functions significantly simplify these functions.\n",
    "    The minimum a Pipulate function must return is this 3-element tuple.\"\"\"\n",
    "    return Response(ok=True, status_code='200', text='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def url(passed_in_func):\n",
    "    \"\"\"This is the frequently-used decorator function that passes along pre-fetched HTML.\n",
    "    \n",
    "    This decorator function allows you to use @url above any Pipulate function.\n",
    "    This in turn allows \"html\" to be used as the argument instead of **kwargs.\n",
    "    Swapping out full HTML for a URL is great for screen-scraping functions like title.\n",
    "    This is also very efficient, because all URL fetches are cached in a database.\n",
    "    This means that multiple scraper functions can be used together efficiently.\n",
    "    This function must appear in this file before anything decorated by it.\"\"\"\n",
    "    def requests_wrapper(**row_dict):\n",
    "        html = row_dict['response'].text\n",
    "        return passed_in_func(html=html)\n",
    "    return requests_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@url\n",
    "def Title(html):\n",
    "    \"\"\"This is the quintessential example of grabbing a title tag from a URL.\n",
    "\n",
    "    This function does not need **kwargs as it's argument BECAUSE it's decorated.\n",
    "    The @url decorator pre-filters **kwargs and sends along only the pre-fetched html.\n",
    "    The HTML is cached for efficiency on subsequent calls of the same URL.\n",
    "    This pattern can be used for extracting any TEXT NODE from HTML (not attributes).\n",
    "    Pipulate functions are case insensitive, so this could also be 'title'.\"\"\"\n",
    "    return extract_text_node(html=html, tag='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def realurl(**row_dict):\n",
    "    \"\"\"Returns the actual URL retrieved after resolving all redirects.\"\"\"\n",
    "    url = row_dict['url']\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        return Response(ok=True, status_code='200', text=response.url)\n",
    "    except:\n",
    "        return Response(ok=True, status_code='200', text=response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apexdomain(**row_dict):\n",
    "    \"\"\"Usually returns the apex or registered domain, given an URL.\"\"\"\n",
    "    path = row_dict['url']\n",
    "    if path:\n",
    "        apex = urlparse(path).hostname.split(\".\")\n",
    "        try:\n",
    "            apex = \".\".join(len(apex[-2]) < 4 and apex[-3:] or apex[-2:])\n",
    "            return Response(ok=True, status_code='200', text=apex)\n",
    "        except:\n",
    "            return Response(ok=True, status_code='200', text=\"Can't find\")\n",
    "    else:\n",
    "        return Response(ok=True, status_code='400', text='No input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gaorganic(**kwargs):\n",
    "    path = urlparse(kwargs['url']).path\n",
    "    path = \"ga:pagePath==%s\" % path.replace(\",\", \"\\,\")\n",
    "    service = goodsheet.create_google_service(api_name=\"analytics\", version=\"v3\")\n",
    "    ga_request = service.data().ga().get(\n",
    "        ids=kwargs['ids'],\n",
    "        start_date=kwargs['startdate'],\n",
    "        end_date=kwargs['enddate'],\n",
    "        metrics='ga:organicSearches',\n",
    "        dimensions='ga:pagePath',\n",
    "        filters=path,\n",
    "        start_index='1',\n",
    "        max_results='100'\n",
    "    )\n",
    "    try:\n",
    "        ga_response = ga_request.execute()\n",
    "    except:\n",
    "        return Response(ok=False, status_code='500', text=\"Did not execute\")\n",
    "    if ga_response and 'rows' in ga_response:\n",
    "        return Response(ok=True, status_code='200', text=ga_response['rows'][0][1])\n",
    "    else:\n",
    "        return Response(ok=False, status_code='200', text='Not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work in Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def verbatim100(**kwargs):\n",
    "    from urllib.parse import quote_plus\n",
    "    title = '\"%s\"' % kwargs['title']\n",
    "    endpoint = 'https://www.google.com/search'\n",
    "    search = '%s?num=100&tbs=li%%3A1&q=%s' % (endpoint, quote_plus(title))\n",
    "    try:\n",
    "        response = url_cacher(search, 'verbatim100')\n",
    "        return Response(ok=True, status_code='200', text=\"cached\")\n",
    "    except:\n",
    "        print(\"Error retreiving URL.\")\n",
    "        return Response(ok=False, status_code='500', text=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def serpinspector(**kwargs):\n",
    "    if 'title' in kwargs:\n",
    "        title = kwargs['title']\n",
    "    else:\n",
    "        return Response(ok=True, status_code='500', text='None')\n",
    "    search = buildquery(title, rpp=100, quote=True)\n",
    "    pattern = re.compile('<a href=\\\"/url\\?q=(.*?)&')\n",
    "    with shelve.open('verbatim100') as db:\n",
    "        if search in db.keys():\n",
    "            response = db[search]\n",
    "            landing_pages = re.findall(pattern, response.text)\n",
    "            landing_pages = set([urlparse(x).hostname for x in landing_pages])\n",
    "            return Response(ok=True, status_code='200', text=landing_pages)\n",
    "        else:\n",
    "            return Response(ok=True, status_code='400', text=\"Not Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparesets(**kwargs):\n",
    "    if 'serpinspector' in kwargs:\n",
    "        a_set = kwargs['serpinspector']\n",
    "        if isinstance(a_set, str):\n",
    "            a_set = eval(a_set)\n",
    "    else:\n",
    "        return Response(ok=True, status_code='500', text='{}')\n",
    "    check_list = set([urlparse(x).hostname for x in private.monitor_list()])\n",
    "    intersection = a_set.intersection(check_list)\n",
    "    if intersection:\n",
    "        return Response(ok=True, status_code='200', text=intersection)\n",
    "    else:\n",
    "        return Response(ok=True, status_code='200', text=\"{}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildquery(title, rpp=100, quote=False):\n",
    "    if quote:\n",
    "        title = '\"%s\"' % title\n",
    "    endpoint = 'https://www.google.com/search'\n",
    "    query = '%s?num=%s&tbs=li%%3A1&q=%s' % (endpoint, rpp, quote_plus(title))\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extracthit(**kwargs):\n",
    "    if 'title' in kwargs:\n",
    "        title = kwargs['title']\n",
    "    else:\n",
    "        return Response(ok=True, status_code='500', text='None')\n",
    "    search = buildquery(title, rpp=100, quote=True)\n",
    "    pattern = re.compile('<a href=\\\"/url\\?q=(.*?)&')\n",
    "    response = cached_url(search, 'verbatim100')\n",
    "    landing_pages = re.findall(pattern, response.text)\n",
    "    if 'serpinspector' in kwargs:\n",
    "        a_set = kwargs['comparesets']\n",
    "        if isinstance(a_set, str):\n",
    "            a_set = eval(a_set)\n",
    "    hits = set()\n",
    "    for spot_me in a_set:\n",
    "        for spot_in in landing_pages:\n",
    "            if spot_me in spot_in:\n",
    "                hits.add(spot_in)\n",
    "    return Response(ok=True, status_code='200', text=hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cached_url(url, dbname):\n",
    "    with shelve.open(dbname) as db:\n",
    "        if url in db.keys():\n",
    "            return db[url]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def url_cacher(url, dbname):\n",
    "    from time import sleep\n",
    "    with shelve.open(dbname) as db:\n",
    "        if url in db.keys():\n",
    "            response = db[url]\n",
    "        else:\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                response = None\n",
    "        if response and response.ok:\n",
    "            db[url] = response\n",
    "        else:\n",
    "            print(\"Sleeping 5 minutes before retrying.\")\n",
    "            sleep(300)\n",
    "            return False\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate() funcs return lists of lists [['A1','B1','C1'],['A2','B2','C2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def populate_from_gsc(site, start, end):\n",
    "    service = goodsheet.create_google_service(api_name=\"webmasters\", version=\"v3\")\n",
    "    listoflists = []\n",
    "    units = 5000\n",
    "    limit = 4\n",
    "    for its, aset in enumerate(range(0, units*limit, units)):\n",
    "        sofar = (its+1)*units\n",
    "        print(\"Fetching %s of %s from Search Console...\" % (sofar, units*limit))\n",
    "        mydata = None\n",
    "        request = {\n",
    "            'startDate': str(start),\n",
    "            'endDate': str(end),\n",
    "            'dimensions': ['query', 'page'],\n",
    "            'rowLimit': str(units),\n",
    "            'startRow': str(aset)\n",
    "        }\n",
    "        mydata = service.searchanalytics().query(siteUrl=site, body=request).execute()\n",
    "        if mydata and 'rows' in mydata:\n",
    "            listoflists.append(mydata['rows'])\n",
    "        else:\n",
    "            break\n",
    "    too_many_dicts = sum(listoflists, [])\n",
    "    list_of_rows = []\n",
    "    print(\"Transforming dicts to lists\", end=\"\")\n",
    "    for index, item in enumerate(too_many_dicts):\n",
    "        if index%1000 == 0:\n",
    "            print('.', end='')\n",
    "        row = [item['keys'][0], item['keys'][1], item['position'], \n",
    "               item['impressions'], item['clicks'], item['ctr']]\n",
    "        list_of_rows.append(row)\n",
    "        list_of_rows.sort(key=lambda x: (-x[4], -x[3]))\n",
    "    columns = [['keyword', 'url', 'position', 'impressions', 'clicks', 'ctr']]\n",
    "    print(\"Returning a big list!\")\n",
    "    return columns + list_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def populate_from_ga(prepath, profileid, start, end):\n",
    "    \"\"\"Experiment at https://ga-dev-tools.appspot.com/query-explorer/\"\"\"\n",
    "    service = create_google_service(api_name=\"analytics\", version=\"v3\")\n",
    "    ga_request = service.data().ga().get(\n",
    "        ids=profileid,\n",
    "        start_date=start,\n",
    "        end_date=end,\n",
    "        metrics='ga:organicSearches,ga:sessions,ga:bounces',\n",
    "        dimensions='ga:pagePath',\n",
    "        sort='-ga:organicSearches,-ga:sessions',\n",
    "        filters='ga:organicSearches>0',\n",
    "        samplingLevel='HIGHER_PRECISION',\n",
    "        start_index='1',\n",
    "        max_results='10000'\n",
    "    )\n",
    "    try:\n",
    "        ga_response = ga_request.execute()\n",
    "    except:\n",
    "        return [['ERROR']]\n",
    "    if 'rows' in ga_response:\n",
    "        raw_rows = ga_response['rows']\n",
    "        list_of_lists = [['url', 'organicsearches', 'sessions', 'bounces']]+[[prepath+a,b,c,d] for a,b,c,d in raw_rows]\n",
    "        return list_of_lists\n",
    "    else:\n",
    "        return [['ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_page_crawl(url, regex=None):\n",
    "    \"\"\"Returns a set of all site URLs found on a page, optionally filtered.\"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "    except:\n",
    "        return [['ERROR']]\n",
    "    lookfor = \"%s://%s/\" % (urlparse(url).scheme, urlparse(url).hostname)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    all_links = soup.find_all(\"a\")\n",
    "    nodupes = set()\n",
    "    for link in all_links:\n",
    "        url_fragment = link.get(\"href\")\n",
    "        href = urljoin(url, url_fragment)\n",
    "        if (type(href).__name__ == 'str' and len(href) > len(lookfor)\n",
    "            and lookfor in href and href not in nodupes):\n",
    "            if regex:\n",
    "                match = re.search(regex, href, flags=re.I)\n",
    "                if match:\n",
    "                    nodupes.add(href)\n",
    "            elif not regex:\n",
    "                nodupes.add(href)\n",
    "    if nodupes:\n",
    "        columns = [['URL']]\n",
    "        list_of_lists = [[x] for x in nodupes]\n",
    "        return columns + list_of_lists\n",
    "    else:\n",
    "        return [['ERROR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def populate_from_atom(sheet_name, tab_name, feed_url):\n",
    "    try:\n",
    "        worksheet = goodsheet.oauth().open(sheet_name).worksheet(tab_name)\n",
    "        rows = worksheet.row_count\n",
    "        cols = worksheet.col_count\n",
    "        end_range = worksheet.get_addr_int(rows, cols)\n",
    "    except:\n",
    "        return [['ERROR']]\n",
    "    try:\n",
    "        feed = requests.get(feed_url).text\n",
    "    except:\n",
    "        return None\n",
    "    first_row = 2\n",
    "    if rows > 100:\n",
    "        first_row = rows - 100\n",
    "    start_range = worksheet.get_addr_int(first_row, 1)\n",
    "    row_range = '%s:%s' % (start_range, end_range)\n",
    "    cell_range = worksheet.range(row_range)\n",
    "    check_for_these_urls = [x.value for x in cell_range][1::3]\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(feed, \"lxml\")\n",
    "    entries = soup.find_all(\"entry\")\n",
    "    list_of_entries = []\n",
    "    for entry in entries:\n",
    "        title = entry.find(\"title\").text\n",
    "        link = entry.find(\"link\").attrs['href']\n",
    "        updated = entry.find(\"updated\").text\n",
    "        if link not in check_for_these_urls:\n",
    "            list_of_entries.append([title, link, updated])\n",
    "    return list_of_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Functions that are not called from sheets or workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text_node(html, tag):\n",
    "    \"\"\"This is the helper function that extracts text-nodes like title tags from HTML.\n",
    "    \n",
    "    This function is for simple text-node screen scraping, such as for the Title tag.\n",
    "    It is different from many support functions in that the return values get wrapped\n",
    "    in the same Response() named tuple as functions that are meant to be called from \n",
    "    spreadsheets directly (like Title), so that those can be short and clear like this:\n",
    "    \n",
    "        @url\n",
    "        def Title(html):\n",
    "            return extract_text_node(html=html, tag=\"title\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if not html or not tag:\n",
    "        return Response(ok=False, status_code='400', text=None)\n",
    "    pattern = r'<{0}\\s?>(.*?)</{0}\\s?>'.format(tag.lower())\n",
    "    compiled = re.compile(pattern=pattern, flags=re.DOTALL)\n",
    "    matches = compiled.findall(string=html)\n",
    "    if matches:\n",
    "        text = matches[0].strip()\n",
    "        return Response(ok=True, status_code='200', text=text)\n",
    "    else:\n",
    "        return Response(ok=True, status_code='200', text=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    \"\"\"http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "    \n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def normalize_whitespace(string):\n",
    "    return re.sub('[\\s\\r\\n]+', ' ', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class common_date_boundaries():\n",
    "    \"\"\"Create a selection of dates that can be used as arguments to Analytics or Search Console\"\"\"\n",
    "    def __init__(self):\n",
    "        from datetime import date, time, datetime, timedelta\n",
    "        shift = 2\n",
    "        today = date.today()\n",
    "        yesterday = today - timedelta(days=1)\n",
    "        day = today - timedelta(days=shift)\n",
    "        week = day.isocalendar()[1]\n",
    "        year = day.year\n",
    "        first_day_this_month = day.replace(day=1)\n",
    "        first_week_of_year = date(year,1,1)\n",
    "        if(first_week_of_year.weekday()>3):\n",
    "            first_week_of_year = first_week_of_year+timedelta(7-first_week_of_year.weekday())\n",
    "        else:\n",
    "            first_week_of_year = first_week_of_year - timedelta(first_week_of_year.weekday())\n",
    "        prior_weeks_of_year = timedelta(days = (week-1)*7)\n",
    "        week_start = first_week_of_year + prior_weeks_of_year\n",
    "        week_end = first_week_of_year + prior_weeks_of_year + timedelta(days=6)\n",
    "        week_start = week_start + timedelta(days=-1)\n",
    "        week_end = week_end + timedelta(days=-1)\n",
    "        month_end = first_day_this_month - timedelta(days=1)\n",
    "        month_start = month_end.replace(day=1)\n",
    "        month_days = (month_end - month_start).days+1\n",
    "        start_30_days = day - timedelta(days=30)\n",
    "        end_30_days = day\n",
    "        start_90_days = day - timedelta(days=90) - timedelta(days=shift-1)\n",
    "        end_90_days = day\n",
    "        props = ['month_days','year','week',\n",
    "                 'first_week_of_year',\n",
    "                 'today','day',\n",
    "                 'first_day_this_month',\n",
    "                 'month_start','month_end',\n",
    "                 'week_start','week_end',\n",
    "                 'start_30_days','end_30_days',\n",
    "                 'start_90_days','end_90_days'\n",
    "                ]\n",
    "        for prop in props:\n",
    "            command = \"self.%s = str(%s)\" % (prop, prop)\n",
    "            exec(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topsites():\n",
    "    import requests, re\n",
    "    print(\"Fetching the Alexa Top 25 US sites...\")\n",
    "    site = 'http://www.alexa.com/topsites/countries/US'\n",
    "    response = requests.get(site)\n",
    "    unresolved_urls = re.findall('/siteinfo/(.*)?\"', response.text)\n",
    "    responses = [requests.get('http://'+x) for x in unresolved_urls]\n",
    "    details = [[[y.status_code for y in x.history], x.status_code, x.url] for x in responses]\n",
    "    flat = [(x[0], x[1][0], x[1][1], x[1][2], '', '') for x in list(zip(unresolved_urls, details))]\n",
    "    returnme = [('original_url', 'redirect_chain', 'status_code', \n",
    "                 'resolved_url', 'proxy_cache', 'cached_title')] + flat\n",
    "    return returnme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def endless_proxies():\n",
    "    global proxies\n",
    "    print('. ', end=\"\")\n",
    "    if not proxies:\n",
    "        print(\"(Fetching and shuffling web proxies...) \", end=\"\")\n",
    "        proxies = proxy_generator(31)\n",
    "    try:\n",
    "        proxy = next(proxies)\n",
    "    except StopIteration:\n",
    "        print(\"Ran out of good proxies. Re-creating generator.\")\n",
    "        proxies = proxy_generator(31)\n",
    "        proxy = next(proxies)\n",
    "    yield proxy\n",
    "\n",
    "def proxy_generator(num_pages):\n",
    "    import hma_scraper\n",
    "    from random import shuffle\n",
    "    from collections import namedtuple\n",
    "    from urllib.parse import urlparse\n",
    "    import requests\n",
    "    from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "    requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "    Proxy = namedtuple('Proxy', 'good scheme address seconds')\n",
    "    proxy_list = hma_scraper.scrape_hma(num_pages)\n",
    "    shuffle(proxy_list)\n",
    "    for a_proxy in proxy_list:\n",
    "        url_parts = urlparse(a_proxy)\n",
    "        requests_arg = {url_parts.scheme: a_proxy}\n",
    "        try:\n",
    "            result = requests.get('https://www.google.com/search?q=test', \n",
    "                                  proxies=requests_arg, verify=False, timeout=1)\n",
    "            yield Proxy(True, url_parts.scheme, a_proxy, result.elapsed.total_seconds())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def proxy_cache(**kwargs):\n",
    "    resolved_url = kwargs['resolved_url']\n",
    "    import requests\n",
    "    from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "    requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "    global proxy\n",
    "    if proxy:\n",
    "        proxy_arg = {proxy.scheme: proxy.address}\n",
    "        try:\n",
    "            response = requests.get(resolved_url, proxies=proxy_arg, timeout=5)\n",
    "            return Response(ok=True, status_code='200', text=proxy.address)\n",
    "        except:\n",
    "            pass\n",
    "    proxy = next(endless_proxies())\n",
    "    proxy_arg = {proxy.scheme: proxy.address}\n",
    "    for i in range(100):\n",
    "        try:\n",
    "            response = requests.get(resolved_url, proxies=proxy_arg, timeout=5)\n",
    "            break\n",
    "        except:\n",
    "            proxy = next(endless_proxies())\n",
    "            proxy_arg = {proxy.scheme: proxy.address}\n",
    "        else:\n",
    "            print(\"It seems no proxies are working.\")\n",
    "            raise SystemExit()\n",
    "    #return_tuple = proxy.address, response.status_code, len(response.text)\n",
    "    return Response(ok=True, status_code='200', text=proxy.address)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
