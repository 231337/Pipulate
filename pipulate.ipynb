{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions for plugging into Pipulate-frameworks for conducting SEO investigations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests, re, os\n",
    "from collections import namedtuple\n",
    "from html.parser import HTMLParser\n",
    "from datetime import date, time, datetime, timedelta\n",
    "import notebook_finder\n",
    "import goodsheet\n",
    "import pandas as pd\n",
    "Response = namedtuple('Response', 'ok status_code text') # Shape the standard pipulate response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pipulate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def foo(**kwargs):\n",
    "    \"\"\"This is a bare-bones copy-and-paste example for new Pipulate functions.\n",
    "    \n",
    "    Functions in this file can be used as column names in Google Sheets.\n",
    "    Text output from these functions get inserted into the spreadsheet.\n",
    "    Pipulate functions must have **kwargs unless preceded by a decorator.\n",
    "    Decorators and support-functions significantly simplify these functions.\n",
    "    The minimum a Pipulate function must return is this 3-element tuple.\"\"\"\n",
    "    return Response(ok=True, status_code='200', text=\"bar2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def url(passed_in_func):\n",
    "    \"\"\"This is the frequently-used decorator function that passes along pre-fetched HTML.\n",
    "    \n",
    "    This decorator function allows you to use @url above any Pipulate function.\n",
    "    This in turn allows \"html\" to be used as the argument instead of **kwargs.\n",
    "    Swapping out full HTML for a URL is great for screen-scraping functions like title.\n",
    "    This is also very efficient, because all URL fetches are cached in a database.\n",
    "    This means that multiple scraper functions can be used together efficiently.\n",
    "    This function must appear in this file before anything decorated by it.\"\"\"\n",
    "    def requests_wrapper(**row_dict):\n",
    "        html = row_dict['response'].text\n",
    "        return passed_in_func(html=html)\n",
    "    return requests_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@url\n",
    "def Title(html):\n",
    "    \"\"\"This is the quintessential example of grabbing a title tag from a URL.\n",
    "    \n",
    "    This function features a decorator AND a support function.\n",
    "    The @url decorator pre-filters **kwargs and sends along pre-fetched html.\n",
    "    The HTML is cached for efficiency on subsequent calls of the same URL.\n",
    "    This pattern can be repeated for extracting any text node from HTML.\n",
    "    Pipulate functions are case insensitive, so this could also be 'title'.\n",
    "    Here we see us returning the output of a helper function.\"\"\"\n",
    "    return extract_text_node(html=html, tag=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text_node(html, tag):\n",
    "    \"\"\"This is the helper function that extracts text-nodes like title tags from HTML.\n",
    "    \n",
    "    This function is for simple text-node screen scraping, such as needed for title.\n",
    "    Text nodes are different from HTML arguments like hrefs, metas and canonicals.\n",
    "    When fed HTML and some \"enclosing\" element name, it will return the first instance.\n",
    "    The Python value None will be returned if no match is found, but with a success code.\n",
    "    This is because the function executed successfuly, only that no match was found.\"\"\"\n",
    "    \n",
    "    if not html or not tag:\n",
    "        return Response(ok=False, status_code='400', text=None)\n",
    "    pattern = r'<{0}\\s?>(.*?)</{0}\\s?>'.format(tag.lower())\n",
    "    compiled = re.compile(pattern=pattern, flags=re.DOTALL)\n",
    "    matches = compiled.findall(string=html)\n",
    "    if matches:\n",
    "        text = matches[0].strip()\n",
    "        return Response(ok=True, status_code='200', text=text)\n",
    "    else:\n",
    "        return Response(ok=True, status_code='200', text=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Services Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_google_service(filename, api_name, version):\n",
    "    \"\"\"This lets you create instances of Google Services.\"\"\"\n",
    "    \n",
    "    import httplib2\n",
    "    from oauth2client import file, tools\n",
    "    from apiclient.discovery import build\n",
    "    path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    filename = '%s/%s.dat' % (path, filename)\n",
    "    storage = file.Storage(filename)\n",
    "    credentials = storage.get()\n",
    "    http = credentials.authorize(http = httplib2.Http())\n",
    "    service = build(api_name, version, http)\n",
    "    return service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analytics(**kwargs):\n",
    "    service = create_google_service(filename=\"oauth\", api_name=\"analytics\", version=\"v3\")\n",
    "    request = {\n",
    "        'startDate': str(start),\n",
    "        'endDate': str(end),\n",
    "        'dimensions': ['query', 'page'],\n",
    "        'rowLimit': str(units),\n",
    "        'startRow': str(aset)\n",
    "    }\n",
    "    mydata = service.searchanalytics().query(siteUrl=site, body=request).execute()\n",
    "    if mydata and 'rows' in mydata:\n",
    "        listoflists.append(mydata['rows'])\n",
    "    return sum(listoflists, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_email():\n",
    "    \"\"\"May end up moving this into GoodSheet\"\"\"\n",
    "    service = create_google_service(filename=\"oauth\", api_name=\"oauth2\", version=\"v2\")\n",
    "    user_document = service.userinfo().get().execute()\n",
    "    email = user_document['email']\n",
    "    return email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to call from other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser):\n",
    "    \"\"\"http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_whitespace(string):\n",
    "    return re.sub('[\\s\\r\\n]+', ' ', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipulate Functions that can be used as column-labels of spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@url\n",
    "def breadcrumb(html):\n",
    "    pattern = '<ul .*?(class|id)=\"breadcrumb.*?>(?P<return>.*?)</ul>'.format(\"title\")\n",
    "    compiled = re.compile(pattern=pattern, flags=re.DOTALL)\n",
    "    matches = compiled.search(string=html)\n",
    "    if hasattr(matches, 'group'):\n",
    "        text = matches.group('return')\n",
    "    else:\n",
    "        return Response(ok=True, status_code='200', text=None)\n",
    "    scrubbed_text = normalize_whitespace(strip_tags(text))\n",
    "    path_list = scrubbed_text.split(\"/\")\n",
    "    trail = '/'.join([x.strip() for x in path_list])\n",
    "    return Response(ok=True, status_code='200', text=trail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "    #test_url = 'http://mikelev.in/'\n",
    "    #response = requests.get(test_url)\n",
    "    #print(Title(url=test_url, response=response))\n",
    "    #test_url = 'noyb'\n",
    "    #response = requests.get(test_url)\n",
    "    #print(breadcrumb(**{'response' : response}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Work In Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correlate_timerange_averages(site):\n",
    "    \"\"\"Returns a 4-item list of search_console results, one for each date range.\"\"\"\n",
    "    from functools import reduce\n",
    "    dates = make_date_markers()\n",
    "    tupleo_dates = namedtuple('Dates', 'start end')\n",
    "    date_list = []\n",
    "    date_list.append(tupleo_dates(start=dates.day, end=dates.day))\n",
    "    date_list.append(tupleo_dates(start=dates.week_start, end=dates.week_end))\n",
    "    date_list.append(tupleo_dates(start=dates.month_start, end=dates.month_end))\n",
    "    date_list.append(tupleo_dates(start=dates.start_90_days, end=dates.end_90_days))\n",
    "    pre_baked = []\n",
    "    for start, end in date_list:\n",
    "        pre_baked.append(search_console(site, start, end))\n",
    "    joinables = []\n",
    "    for index, a_response in enumerate(pre_baked):\n",
    "        joinables.append([(x['keys'][1],\n",
    "                           x['keys'][0],\n",
    "                           x['position'],\n",
    "                           x['clicks'],\n",
    "                           x['impressions'],\n",
    "                           x['ctr']) for x in pre_baked[index]])\n",
    "    list_of_frames = []\n",
    "    column_suffix = ['dy', 'wk', 'mo', '90']\n",
    "    for index, a_table in enumerate(joinables):\n",
    "        list_of_frames.append(pd.DataFrame(data=a_table, columns=['url', 'keyword',\n",
    "                               'position'+column_suffix[index],\n",
    "                               'clicks'+column_suffix[index],\n",
    "                               'impressions'+column_suffix[index],\n",
    "                               'ctr'+column_suffix[index]]))\n",
    "    joined_results = reduce(lambda l,r: pd.merge(l,r,on=['url', 'keyword'], how=\"outer\"), list_of_frames)\n",
    "    joined_results = joined_results[['url', 'keyword',\n",
    "                                    'positiondy', 'positionwk', 'positionmo', 'position90',\n",
    "                                    'clicksdy', 'clickswk', 'clicksmo', 'clicks90',\n",
    "                                    'impressionsdy', 'impressionswk', 'impressionsmo', 'impressions90',\n",
    "                                    'ctrdy', 'ctrwk', 'ctrmo', 'ctr90']]\n",
    "    joined_results.sort_values('impressionsdy', axis=0, ascending=False, inplace=True)\n",
    "    return joined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class make_date_markers():\n",
    "    \"\"\"Create a selection of dates that can be used as arguments to Analytics or Search Console\"\"\"\n",
    "    def __init__(self):\n",
    "        shift = 3\n",
    "        today = date.today()\n",
    "        day = today - timedelta(days=shift)\n",
    "        week = day.isocalendar()[1]\n",
    "        year = day.year\n",
    "        first_day_this_month = day.replace(day=1)\n",
    "        first_week_of_year = date(year,1,1)\n",
    "        if(first_week_of_year.weekday()>3):\n",
    "            first_week_of_year = first_week_of_year+timedelta(7-first_week_of_year.weekday())\n",
    "        else:\n",
    "            first_week_of_year = first_week_of_year - timedelta(first_week_of_year.weekday())\n",
    "        prior_weeks_of_year = timedelta(days = (week-1)*7)\n",
    "        week_start = first_week_of_year + prior_weeks_of_year\n",
    "        week_end = first_week_of_year + prior_weeks_of_year + timedelta(days=6)\n",
    "        week_start = week_start + timedelta(days=-1)\n",
    "        week_end = week_end + timedelta(days=-1)\n",
    "        month_end = first_day_this_month - timedelta(days=1)\n",
    "        month_start = month_end.replace(day=1)\n",
    "        month_days = (month_end - month_start).days+1\n",
    "        start_90_days = day - timedelta(days=90) - timedelta(days=shift-1)\n",
    "        end_90_days = day\n",
    "        props = ['first_week_of_year',\n",
    "                 'today','day',\n",
    "                 'first_day_this_month',\n",
    "                 'month_start','month_end',\n",
    "                 'week_start','week_end',\n",
    "                 'start_90_days','end_90_days']\n",
    "        for prop in props:\n",
    "            command = \"self.%s = str(%s)\" % (prop, prop)\n",
    "            exec(command)\n",
    "\n",
    "#dates = make_date_markers()\n",
    "#['%s: %s' % (x, eval('dates.%s' % x)) for x in dir(dates) if x[0] != '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_console(site, start, end):\n",
    "    service = create_google_service(filename=\"oauth\", api_name=\"webmasters\", version=\"v3\")\n",
    "    listoflists = []\n",
    "    units = 5000\n",
    "    for aset in range(0, units*4, units):\n",
    "        mydata = None\n",
    "        request = {\n",
    "            'startDate': str(start),\n",
    "            'endDate': str(end),\n",
    "            'dimensions': ['query', 'page'],\n",
    "            'rowLimit': str(units),\n",
    "            'startRow': str(aset)\n",
    "        }\n",
    "        mydata = service.searchanalytics().query(siteUrl=site, body=request).execute()\n",
    "        if mydata and 'rows' in mydata:\n",
    "            listoflists.append(mydata['rows'])\n",
    "        else:\n",
    "            break\n",
    "    return sum(listoflists, [])\n",
    "#dates = make_date_markers()\n",
    "#results = search_console(site='mikelev.in', start=dates.month_start, end=dates.month_end)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
