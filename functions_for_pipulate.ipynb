{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pipulate as gs\n",
    "from itertools import cycle\n",
    "import requests\n",
    "import re\n",
    "\n",
    "use_proxies = False\n",
    "\n",
    "proxy_list = list()\n",
    "bad_blocks = list()\n",
    "\n",
    "try:\n",
    "    with open('goodproxies.txt') as proxies:\n",
    "        for line in proxies:\n",
    "            proxy_list.append(line[:-1])\n",
    "    with open('badproxies.txt') as proxies:\n",
    "        for line in proxies:\n",
    "            bad_blocks.append(line[:-1])\n",
    "    proxy_cycler = cycle(proxy_list)\n",
    "    use_proxies = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Use proxies: %s\" % use_proxies)\n",
    "\n",
    "common_agents = list()\n",
    "with open('useragents.txt') as agents:\n",
    "    for line in agents:\n",
    "        common_agents.append(line[:-1])\n",
    "agent_cycler = cycle(common_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fu():\n",
    "    return 'bar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ga_url(gaid, start, end, path):\n",
    "    \"\"\"Return Google Analytics metrics for a URL.\"\"\"\n",
    "    \n",
    "    # OMG! How do I set all these parameters? Explore at:\n",
    "    # https://ga-dev-tools.appspot.com/query-explorer/\n",
    "    path = path.replace(',', r'\\,')\n",
    "    metrics = ['users', \n",
    "               'newUsers', \n",
    "               'sessions', \n",
    "               'bounceRate',\n",
    "               'pageviewsPerSession', \n",
    "               'avgSessionDuration']\n",
    "    metrics = ''.join(['ga:%s,' % x for x in metrics])[:-1]\n",
    "    ga_request = gs.ga().data().ga().get(\n",
    "        ids='ga:'+gaid,\n",
    "        start_date=start,\n",
    "        end_date=end,\n",
    "        metrics=metrics,\n",
    "        dimensions='ga:pagePath',\n",
    "        sort='-ga:users',\n",
    "        filters='ga:pagePath=='+path,\n",
    "        segment='sessions::condition::ga:medium==organic',\n",
    "        max_results=1\n",
    "    )\n",
    "    ga_response = ga_request.execute()\n",
    "    return_value = []\n",
    "    if 'rows' in ga_response:\n",
    "        raw_rows = ga_response['rows'][0]\n",
    "        return_value = raw_rows\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gsc_keyword(prop, start, end, query):\n",
    "    \"\"\"Return position, clicks & impressions from GSC for keyword.\"\"\"\n",
    "    \n",
    "    request = {\n",
    "      \"startDate\": start,\n",
    "      \"endDate\": end,\n",
    "      \"dimensions\": [\n",
    "        \"query\"\n",
    "      ],\n",
    "      \"dimensionFilterGroups\": [\n",
    "        {\n",
    "          \"filters\": [\n",
    "            {\n",
    "              \"operator\": \"equals\",\n",
    "              \"expression\": query,\n",
    "              \"dimension\": \"query\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    data = gs.gsc().searchanalytics().query(siteUrl='https://www.pcmag.com', body=request).execute()\n",
    "    val = []\n",
    "    if 'rows' in data:\n",
    "        r = data['rows'][0]\n",
    "        val = [start, end] + [r['keys'][0]] + [\n",
    "            r['position']] + [r['clicks']] + [r['impressions']] + [r['ctr']]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gsc_url(prop, start, end, url):\n",
    "    \"\"\"Return position, clicks & impressions from GSC for URL.\"\"\"\n",
    "\n",
    "    request = {\n",
    "      \"startDate\": start,\n",
    "      \"endDate\": end,\n",
    "      \"dimensions\": [\n",
    "        \"page\"\n",
    "      ],\n",
    "      \"dimensionFilterGroups\": [\n",
    "        {\n",
    "          \"filters\": [\n",
    "            {\n",
    "              \"operator\": \"equals\",\n",
    "              \"expression\": url,\n",
    "              \"dimension\": \"page\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    data = gs.gsc().searchanalytics().query(siteUrl='https://www.pcmag.com', body=request).execute()\n",
    "    val = []\n",
    "    if 'rows' in data:\n",
    "        r = data['rows'][0]\n",
    "        val = [start, end] + [r['keys'][0]] + [\n",
    "            r['position']] + [r['clicks']] + [r['impressions']] + [r['ctr']]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gsc_url_keyword(prop, start, end, query, url):\n",
    "    \"\"\"Return position, clicks & impressions from GSC for keyword for URL.\"\"\"\n",
    "\n",
    "    request = {\n",
    "      \"startDate\": start,\n",
    "      \"endDate\": end,\n",
    "      \"dimensions\": [\n",
    "        \"query\",\n",
    "        \"page\"\n",
    "      ],\n",
    "      \"dimensionFilterGroups\": [\n",
    "        {\n",
    "          \"filters\": [\n",
    "            {\n",
    "              \"operator\": \"equals\",\n",
    "              \"expression\": url,\n",
    "              \"dimension\": \"page\"\n",
    "            },\n",
    "            {\n",
    "              \"operator\": \"equals\",\n",
    "              \"expression\": query,\n",
    "              \"dimension\": \"query\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    data = gs.gsc().searchanalytics().query(siteUrl=prop, body=request).execute()\n",
    "    val = []\n",
    "    if 'rows' in data:\n",
    "        r = data['rows'][0]\n",
    "        val = [start] + [r['keys'][0]] + [r['keys'][1]] + [\n",
    "            r['position']] + [r['clicks']] + [r['impressions']] + [r['ctr']]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serp(keyword, rpp=20):\n",
    "    from urllib.parse import quote_plus\n",
    "    global use_proxies, proxy_cycler, bad_blocks, agent_cycler\n",
    "            \n",
    "    print(\"Keyword: %s\" % keyword)\n",
    "    search_url = 'https://www.google.com/search?num=%s&q=%s' % (rpp, quote_plus(keyword))\n",
    "    pattern = re.compile('<h3 class=\"r\"><a href=\"(http.*?)\"')\n",
    "    landing_pages = []\n",
    "    proxied_retries = 3\n",
    "    naked_retries = 1\n",
    "    \n",
    "    # The preferred path if you have anonymous web proxies.\n",
    "    # The light that burns twice as bright burns half as long.\n",
    "    if use_proxies:\n",
    "        for x in range(10):\n",
    "            ip = next(proxy_cycler)\n",
    "            if ip not in bad_blocks:\n",
    "                break\n",
    "            print(\"Couldn't find good proxy. Clear cache and try later.\")\n",
    "            raise SystemExit()\n",
    "        block = '.'.join(ip.split('.')[:3])\n",
    "        proxy = {'http': 'http://'+ip, 'https': 'https://'+ip}\n",
    "        print('Using proxy: %s' % proxy)\n",
    "        agent = next(agent_cycler)\n",
    "        user_agent = {'User-agent': agent}\n",
    "        for i in range(proxied_retries):\n",
    "            try:\n",
    "                response = requests.get(search_url, proxies=proxy, headers=user_agent)\n",
    "                landing_pages = re.findall(pattern, response.text)\n",
    "                break\n",
    "            except:\n",
    "                bad_blocks.append(ip)\n",
    "                agent = next(agent_cycler)\n",
    "                user_agent = {'User-agent': agent}\n",
    "                for x in range(10):\n",
    "                    ip = next(proxy_cycler)\n",
    "                    if ip not in bad_blocks:\n",
    "                        break\n",
    "                print(\"Ran out of good proxies.\")\n",
    "                raise SystemExit()\n",
    "    else:\n",
    "        for i in range(naked_retries):\n",
    "            try:\n",
    "                response = requests.get(search_url, headers=user_agent)\n",
    "                landing_pages = re.findall(pattern, response.text)\n",
    "                break\n",
    "            except:\n",
    "                agent = next(agent_cycler)\n",
    "                user_agent = {'User-agent': agent}\n",
    "        \n",
    "    return landing_pages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
